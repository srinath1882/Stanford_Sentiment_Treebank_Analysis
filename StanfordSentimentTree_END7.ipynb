{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "StanfordSentimentTree_END7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-fobY_VO03i",
        "outputId": "5134f28e-9186-40af-a650-8ba636e6e8ad"
      },
      "source": [
        "import pandas as pd\n",
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb755f90b10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyBJe9qQngMO",
        "outputId": "628ed82a-7fe5-4fab-9c32-3964c674bd86"
      },
      "source": [
        "!pip install googletrans==3.1.0a0\n",
        "import random\n",
        "import googletrans\n",
        "from googletrans import Translator"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/78/be/7b8b99fd74ff5684225f50dd0e865393d2265656ef3b4ba9eaaaffe622b8/rfc3986-1.4.0-py2.py3-none-any.whl\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d3/3c/cdeaf9ab0404853e77c45d9e8021d0d2c01f70a1bb26e460090926fe2a5e/hstspreload-2020.11.21-py3-none-any.whl (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 6.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.11.8)\n",
            "Collecting contextvars>=2.1; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Collecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.1MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.0MB/s \n",
            "\u001b[?25hCollecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.5MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans, contextvars\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp36-none-any.whl size=16369 sha256=36e452b9039d38586fe780c0f00fae91f745ecfd77b74eb978aa071717cc77bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=c2c2fc0e8d071cf66807642a0445810b496329ce20b7fdaa03de984879159e43\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built googletrans contextvars\n",
            "Installing collected packages: immutables, contextvars, sniffio, rfc3986, hstspreload, hpack, hyperframe, h2, h11, httpcore, httpx, googletrans\n",
            "Successfully installed contextvars-2.4 googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.11.21 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 immutables-0.14 rfc3986-1.4.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F71swf6984X3",
        "outputId": "07d37671-acb9-436a-9b8a-638c53486f70"
      },
      "source": [
        "!wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-10 14:11:05--  http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip [following]\n",
            "--2020-12-10 14:11:06--  https://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6372817 (6.1M) [application/zip]\n",
            "Saving to: ‘stanfordSentimentTreebank.zip’\n",
            "\n",
            "stanfordSentimentTr 100%[===================>]   6.08M   689KB/s    in 9.3s    \n",
            "\n",
            "2020-12-10 14:11:16 (672 KB/s) - ‘stanfordSentimentTreebank.zip’ saved [6372817/6372817]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8g7khAQ9Z8I",
        "outputId": "f80bb545-6ecc-4563-cd75-ef16abe1c1a9"
      },
      "source": [
        "!unzip stanfordSentimentTreebank.zip"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  stanfordSentimentTreebank.zip\n",
            "   creating: stanfordSentimentTreebank/\n",
            "  inflating: stanfordSentimentTreebank/datasetSentences.txt  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/stanfordSentimentTreebank/\n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSentences.txt  \n",
            "  inflating: stanfordSentimentTreebank/datasetSplit.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._datasetSplit.txt  \n",
            "  inflating: stanfordSentimentTreebank/dictionary.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._dictionary.txt  \n",
            "  inflating: stanfordSentimentTreebank/original_rt_snippets.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._original_rt_snippets.txt  \n",
            "  inflating: stanfordSentimentTreebank/README.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._README.txt  \n",
            "  inflating: stanfordSentimentTreebank/sentiment_labels.txt  \n",
            "  inflating: __MACOSX/stanfordSentimentTreebank/._sentiment_labels.txt  \n",
            "  inflating: stanfordSentimentTreebank/SOStr.txt  \n",
            "  inflating: stanfordSentimentTreebank/STree.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhFBK_L-JGyq"
      },
      "source": [
        "Preprocess the dataset and derive Train, Test and Validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1WR1L4Hk9bSf"
      },
      "source": [
        "import numpy as np\n",
        "import unicodedata\n",
        "\n",
        "root = \"stanfordSentimentTreebank/\"\n",
        "\n",
        "sentences_file = open(root+\"datasetSentences.txt\")\n",
        "sentences=[]\n",
        "sentences_id = {}\n",
        "for line in sentences_file:\n",
        "\tsentence = line.split(\"\\t\")[1].strip(\"\\n\")\n",
        "\t# sentence = unicode(line.split(\"\\t\")[1].strip(\"\\n\"),\"ISO-8859-1\")\n",
        "\t# sentence = sentence.encode(\"utf-8\")\n",
        "\t# unicodedata.normalize('NFKD', sentence).encode('ascii','ignore')\n",
        "\t# print sentence\n",
        "\tsentences.append(sentence)\n",
        "\tsentences_id[sentence]=line.split(\"\\t\")[0]\n",
        "sentences_file.close()\n",
        "\n",
        "phrase_id_file = open(root+\"dictionary.txt\")\n",
        "phrases={}\n",
        "for line in phrase_id_file:\n",
        "\tline=line.split(\"|\")\n",
        "\tphrases[line[0]]=line[1].strip(\"\\n\")\n",
        "phrase_id_file.close()\n",
        "\n",
        "phraseid_sentiment_file = open(root+\"sentiment_labels.txt\")\n",
        "phrase_sentiments = {}\n",
        "for line in phraseid_sentiment_file:\n",
        "\tline = line.split(\"|\")\n",
        "\tphrase_sentiments[line[0]]=line[1].strip(\"\\n\")\n",
        "phraseid_sentiment_file.close()\n",
        "\n",
        "sentenceid_splitset_file = open(root+\"datasetSplit.txt\")\n",
        "sentenceid_splitset = {}\n",
        "for line in sentenceid_splitset_file:\n",
        "\tline = line.split(\",\")\n",
        "\t# print line\n",
        "\tsentenceid_splitset[line[0]]=line[1].strip(\"\\n\")\n",
        "sentenceid_splitset_file.close()\n",
        "\n",
        "\n",
        "def get_class(label):\n",
        "\tk=float(label)*5.0\n",
        "\tif k>=0 and k<=1:\n",
        "\t\treturn \"0\"\n",
        "\telif k>=1 and k<=2:\n",
        "\t\treturn \"1\"\n",
        "\telif k>=2 and k<=3:\n",
        "\t\treturn \"2\"\n",
        "\telif k>=3 and k<=4:\n",
        "\t\treturn \"3\"\n",
        "\telif k>=4 and k<=5:\n",
        "\t\treturn \"4\"\n",
        "\n",
        "sentence_labels_file = open(\"sentence_labels.txt\",\"w\")\n",
        "train_file = open(\"train.txt\",\"w\")\n",
        "valid_file = open(\"valid.txt\",\"w\")\n",
        "test_file = open(\"test.txt\",\"w\")\n",
        "train_file.write(\"Sentence\\tScore\\tLabel\\n\")\n",
        "valid_file.write(\"Sentence\\tScore\\tLabel\\n\")\n",
        "test_file.write(\"Sentence\\tScore\\tLabel\\n\")\n",
        "for sentence in sentences:\n",
        "\tif sentence in phrases and sentence in sentences_id:\n",
        "\t\tsentence_id = sentences_id[sentence]\n",
        "\t\tsplit_set = sentenceid_splitset[sentence_id]\n",
        "\t\tphrase_id = phrases[sentence]\n",
        "\t\tlabel = phrase_sentiments[phrase_id]\n",
        "\t\tsentence_labels_file.write(sentence+\"\\t\"+label+\"\\n\")\n",
        "\t\tif split_set==\"1\":\n",
        "\t\t\ttrain_file.write(sentence+\"\\t\"+label+\"\\t\"+get_class(label)+\"\\n\")\n",
        "\t\telif split_set==\"2\":\n",
        "\t\t\ttest_file.write(sentence+\"\\t\"+label+\"\\t\"+get_class(label)+\"\\n\")\n",
        "\t\telif split_set==\"3\":\n",
        "\t\t\tvalid_file.write(sentence+\"\\t\"+label+\"\\t\"+get_class(label)+\"\\n\")\n",
        "\n",
        "\t\t# print sentence, label\n",
        "sentence_labels_file.close()\n",
        "train_file.close()\n",
        "valid_file.close()\n",
        "test_file.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GzcGiv_PPhV2",
        "outputId": "4cab5780-db86-453e-c00a-32436c01709e"
      },
      "source": [
        "train_df = pd.read_csv('train.txt',sep='\\t')\n",
        "train_df.head()\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>0.83333</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
              "      <td>0.62500</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>0.72222</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence    Score  Label\n",
              "0  The Rock is destined to be the 21st Century 's...  0.69444      3\n",
              "1  The gorgeously elaborate continuation of `` Th...  0.83333      4\n",
              "2  Singer\\/composer Bryan Adams contributes a sle...  0.62500      3\n",
              "3  You 'd think by now America would have had eno...  0.50000      2\n",
              "4               Yet the act is still charming here .  0.72222      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KntyZT4YcdC3"
      },
      "source": [
        "def random_deletion(sentence, p=0.5): \n",
        "    words = sentence.split()\n",
        "    if len(words) == 1: # return if single word\n",
        "        return ' '.join(words)\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return ' '.join([random.choice(words)])\n",
        "    else:\n",
        "        return ' '.join(remaining)"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zic-venRcewH"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    sentence = sentence.split()\n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return ' '.join(sentence)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV0FLjtel-ZN"
      },
      "source": [
        "translator = Translator()\n",
        "def backTranslate(sentence):\n",
        "  #print(Sentence)\n",
        "  available_langs = list(googletrans.LANGUAGES.keys())\n",
        "  trans_lang = random.choice(available_langs)\n",
        "  translations = translator.translate(sentence, dest=trans_lang) \n",
        "  #print(translations)\n",
        "  t_text = [t.text for t in translations]\n",
        "  translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "  en_text = [t.text for t in translations_en_random]\n",
        "  return ' '.join(en_text)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U4i6k2RImQw"
      },
      "source": [
        "Augment every third data sample randomly. Either random deletion, or random swap or back translate is performed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbwTWH3Lx574"
      },
      "source": [
        "df_len = train_df.shape[0]\n",
        "augmentedList = []\n",
        "# Augment every third review\n",
        "for i in range(0,df_len,3):\n",
        "  datadf = train_df.iloc[i]\n",
        "  index = random.choice([1,2,3])\n",
        "  if index == 1:\n",
        "    result = random_deletion(datadf.Sentence)\n",
        "    #augmentedList.append([result, datadf.Score, datadf.Label])\n",
        "  elif index == 2:\n",
        "    result = random_swap(datadf.Sentence)\n",
        "    #augmentedList.append([result, datadf.Score, datadf.Label])\n",
        "  elif index == 3:\n",
        "    result = backTranslate([datadf.Sentence])\n",
        "  augmentedList.append([result, datadf.Score, datadf.Label])\n",
        "    \n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LCLwk7d4NiVQ",
        "outputId": "559900f4-4768-4ec6-e869-f538a665a2a4"
      },
      "source": [
        "augmentedData = pd.DataFrame(augmentedList,columns=['Sentence','Score','Label'])\n",
        "augmentedData.head()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined the be Conan 21st Century...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>You 'd by America had of eccentrics hearts gold</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Just the in involved this creating the layered...</td>\n",
              "      <td>0.87500</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>. Extreme Ops expectations exceeds `` ''</td>\n",
              "      <td>0.73611</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dramas this .</td>\n",
              "      <td>0.80556</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence    Score  Label\n",
              "0  The Rock is destined the be Conan 21st Century...  0.69444      3\n",
              "1    You 'd by America had of eccentrics hearts gold  0.50000      2\n",
              "2  Just the in involved this creating the layered...  0.87500      4\n",
              "3           . Extreme Ops expectations exceeds `` ''  0.73611      3\n",
              "4                                      Dramas this .  0.80556      4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r1PsOeSENxkZ",
        "outputId": "91b5dee6-a196-4985-d2fb-ed60e6a3c0f3"
      },
      "source": [
        "train_df_copy = train_df\n",
        "train_df = pd.concat([train_df, augmentedData], ignore_index = True) \n",
        "train_df.reset_index() \n",
        "train_df.head()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "      <th>Score</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>0.69444</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>0.83333</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
              "      <td>0.62500</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You 'd think by now America would have had eno...</td>\n",
              "      <td>0.50000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Yet the act is still charming here .</td>\n",
              "      <td>0.72222</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Sentence    Score  Label\n",
              "0  The Rock is destined to be the 21st Century 's...  0.69444      3\n",
              "1  The gorgeously elaborate continuation of `` Th...  0.83333      4\n",
              "2  Singer\\/composer Bryan Adams contributes a sle...  0.62500      3\n",
              "3  You 'd think by now America would have had eno...  0.50000      2\n",
              "4               Yet the act is still charming here .  0.72222      3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npBjn5m1IPSQ",
        "outputId": "bf70fedd-d889-4134-9346-a66ab2b2a61b"
      },
      "source": [
        "train_df.shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10828, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVesLDcsI2bv"
      },
      "source": [
        "Write the augment data to new file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRPqqWMNPGDB"
      },
      "source": [
        "train_df.to_csv('train.csv')"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mLEFYtqalM4"
      },
      "source": [
        "\n",
        "Sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "Label = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)\n",
        "def transformData(file,sep='\\t'):\n",
        "  df = pd.read_csv(file,sep=sep)\n",
        "  # shuffle the DataFrame rows \n",
        "  df = df.sample(frac = 1).reset_index(drop=True)\n",
        "  fields = [('Sentence', Sentence),('Label',Label)]\n",
        "  example = [data.Example.fromlist([df.Sentence[i],df.Label[i]], fields) for i in range(df.shape[0])] \n",
        "  dataset = data.Dataset(example, fields)\n",
        "  return dataset"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-7hRw2_bDF8"
      },
      "source": [
        "train = transformData('train.txt')\n",
        "#train = transformData('train.csv',sep=',')\n",
        "valid = transformData('valid.txt')\n",
        "test = transformData('test.txt')"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRCFzvCjpu03",
        "outputId": "5180151b-0462-474f-f629-261d00e3e818"
      },
      "source": [
        "vars(train.examples[11])"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Label': 2,\n",
              " 'Sentence': ['Veers',\n",
              "  'uncomfortably',\n",
              "  'close',\n",
              "  'to',\n",
              "  'pro',\n",
              "  '-',\n",
              "  'Serb',\n",
              "  'propaganda',\n",
              "  '.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dys-Ssv-aDVw"
      },
      "source": [
        "Sentence.build_vocab(train)\n",
        "Label.build_vocab(train)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kvfBcYgb6si",
        "outputId": "968ea103-df3c-4c71-de70-bfb8eeb62c9d"
      },
      "source": [
        "print('Size of input vocab : ', len(Sentence.vocab))\n",
        "print('Size of label vocab : ', len(Label.vocab))\n",
        "print('Top 10 words appreared repeatedly :', list(Sentence.vocab.freqs.most_common(10)))\n",
        "print('Labels : ', Label.vocab.stoi)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  16524\n",
            "Size of label vocab :  5\n",
            "Top 10 words appreared repeatedly : [('.', 7637), (',', 6706), ('the', 5714), ('of', 4170), ('and', 4152), ('a', 4147), ('to', 2844), ('-', 2566), ('is', 2403), (\"'s\", 2354)]\n",
            "Labels :  defaultdict(<function _default_unk_index at 0x7fb707d101e0>, {3: 0, 1: 1, 2: 2, 4: 3, 0: 4})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbdSlj44b-O6"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_cx7Qb7cLT-"
      },
      "source": [
        "train_iterator, valid_iterator,test_iterator = data.BucketIterator.splits((train, valid,test), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.Sentence),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNP32TGOcTAC"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Sentence.vocab.stoi, tokens)"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4LYybZJ2DJd"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "        embedded = self.dropout(embedded)\n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        #dense_outputs = self.dropout(dense_outputs)\n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbbjzfcaiqh1",
        "outputId": "e11416d3-c62a-4c96-840a-0d61952b78b8"
      },
      "source": [
        "len(Sentence.vocab)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16524"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIwoFpO5f93f"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(Sentence.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 512\n",
        "num_output_nodes = len(Label.vocab)\n",
        "num_layers = 2\n",
        "dropout = 0.5\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLlu82tlgEbI",
        "outputId": "eecc59df-430c-4fce-9a4e-faabdc98efff"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(16524, 300)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (encoder): LSTM(300, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
            ")\n",
            "The model has 8,728,085 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH3fTID5gHL9"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60rZwN4OgJ8x"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        Sentence, Sentence_lengths = batch.Sentence   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(Sentence, Sentence_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.Label)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.Label)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXz3q1_rgJ2o"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            Sentence, Sentence_lengths = batch.Sentence\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(Sentence, Sentence_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.Label)\n",
        "            acc = binary_accuracy(predictions, batch.Label)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1r5wSMn_gJwp",
        "outputId": "0685b0d2-4040-45af-add3-6a6667a32d9f"
      },
      "source": [
        "N_EPOCHS = 15\n",
        "best_valid_loss = float('inf')\n",
        "batch_size = 32\n",
        "f = open(\"results_lstm.txt\",'a')\n",
        "f.write(f'Start Training the Model\\n\\n')\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')\n",
        "    f.write(\"batch_size : \"+str(batch_size)+\"  nb_epoch : \"+str(epoch)+\"   \"+\"optimizer : adam\"+\"\\n\")\n",
        "    f.write(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    f.write(f'\\t Vl. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 1.578 | Train Acc: 27.03%\n",
            "\t Val. Loss: 1.576 |  Val. Acc: 26.83% \n",
            "\n",
            "\tTrain Loss: 1.567 | Train Acc: 29.66%\n",
            "\t Val. Loss: 1.569 |  Val. Acc: 29.84% \n",
            "\n",
            "\tTrain Loss: 1.553 | Train Acc: 32.57%\n",
            "\t Val. Loss: 1.543 |  Val. Acc: 33.89% \n",
            "\n",
            "\tTrain Loss: 1.515 | Train Acc: 36.72%\n",
            "\t Val. Loss: 1.539 |  Val. Acc: 34.14% \n",
            "\n",
            "\tTrain Loss: 1.477 | Train Acc: 41.44%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.72% \n",
            "\n",
            "\tTrain Loss: 1.441 | Train Acc: 45.31%\n",
            "\t Val. Loss: 1.522 |  Val. Acc: 36.41% \n",
            "\n",
            "\tTrain Loss: 1.401 | Train Acc: 49.70%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 35.03% \n",
            "\n",
            "\tTrain Loss: 1.376 | Train Acc: 52.26%\n",
            "\t Val. Loss: 1.531 |  Val. Acc: 35.76% \n",
            "\n",
            "\tTrain Loss: 1.340 | Train Acc: 56.02%\n",
            "\t Val. Loss: 1.536 |  Val. Acc: 35.07% \n",
            "\n",
            "\tTrain Loss: 1.311 | Train Acc: 59.36%\n",
            "\t Val. Loss: 1.553 |  Val. Acc: 33.06% \n",
            "\n",
            "\tTrain Loss: 1.294 | Train Acc: 60.92%\n",
            "\t Val. Loss: 1.545 |  Val. Acc: 34.57% \n",
            "\n",
            "\tTrain Loss: 1.268 | Train Acc: 63.36%\n",
            "\t Val. Loss: 1.529 |  Val. Acc: 35.20% \n",
            "\n",
            "\tTrain Loss: 1.253 | Train Acc: 64.99%\n",
            "\t Val. Loss: 1.535 |  Val. Acc: 34.55% \n",
            "\n",
            "\tTrain Loss: 1.228 | Train Acc: 67.73%\n",
            "\t Val. Loss: 1.538 |  Val. Acc: 35.67% \n",
            "\n",
            "\tTrain Loss: 1.212 | Train Acc: 69.55%\n",
            "\t Val. Loss: 1.542 |  Val. Acc: 34.09% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8oMO0O1gJpd",
        "outputId": "56676a15-c020-4367-a6a6-421f7aed7fcd"
      },
      "source": [
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "print(f'\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}% \\n')\n",
        "f.write(f'\\n\\nTesting the Model')\n",
        "f.write(f'\\n\\n\\n\\t Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}% \\n')\n",
        "f.close()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t Test Loss: 1.514 |  Test Acc: 37.89% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdLs3R1IPKia"
      },
      "source": [
        ""
      ],
      "execution_count": 87,
      "outputs": []
    }
  ]
}